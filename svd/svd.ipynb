{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD构建词向量\n",
    "把路径参数调好直接运行即可，20min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去除停用词和恢复词语原型\n",
      "201626\n",
      "加载共现矩阵\n",
      "svd分解\n",
      "S(200,),[  1098.04519633   1100.45801202   1109.25252177   1115.81023497\n",
      "   1124.2493256    1128.53775306   1131.48622134   1139.68639495\n",
      "   1142.25493929   1142.61110648   1147.37858439   1152.17372785\n",
      "   1153.31908651   1159.04543802   1159.19100021   1168.30065258\n",
      "   1168.74869896   1175.62459389   1181.06811785   1185.67339774\n",
      "   1188.50931537   1193.90747844   1197.90667399   1205.9779365\n",
      "   1206.42233614   1210.81906955   1216.08462039   1223.77871782\n",
      "   1229.85136215   1236.07631286   1243.92818657   1248.64232099\n",
      "   1254.02277213   1258.75726801   1267.45374673   1278.10360228\n",
      "   1278.91278103   1280.84186995   1293.08385585   1302.07520185\n",
      "   1305.71604183   1311.94784112   1323.43885175   1325.82319105\n",
      "   1329.76523891   1335.40607773   1338.70593561   1349.11053843\n",
      "   1350.16395907   1351.79095115   1362.80055161   1364.19580763\n",
      "   1378.76993376   1381.85990783   1391.36590329   1404.10255849\n",
      "   1417.11904892   1421.38461249   1422.01462217   1425.98492248\n",
      "   1426.16352494   1437.02099885   1453.78239119   1457.18277683\n",
      "   1463.84969551   1468.0616666    1469.68476373   1479.45180359\n",
      "   1482.40945046   1496.03487886   1504.29192169   1507.82337254\n",
      "   1512.66549001   1531.77911142   1536.52457745   1547.33523169\n",
      "   1560.03320262   1562.96207045   1572.14763644   1574.81927335\n",
      "   1589.2039716    1592.35850809   1597.21459455   1633.96153149\n",
      "   1634.5307535    1641.76814761   1652.42695668   1657.15010293\n",
      "   1664.92843405   1669.66090422   1679.68244293   1684.58439348\n",
      "   1690.73020776   1695.56439967   1712.41742102   1715.0548179\n",
      "   1725.23181025   1730.6572743    1751.05434295   1756.04362475\n",
      "   1776.00938677   1781.64073472   1782.88630837   1785.31106334\n",
      "   1801.32560836   1812.75111905   1832.73794463   1845.48275976\n",
      "   1863.42534349   1870.89745782   1880.32214646   1893.39526053\n",
      "   1919.4546807    1933.78916252   1955.52897085   1968.9509638\n",
      "   1971.5413693    1976.6358964    1991.82112882   2005.09072821\n",
      "   2075.62697366   2078.03150199   2105.07446097   2129.51635574\n",
      "   2147.41374384   2157.9689229    2194.05702802   2216.75227084\n",
      "   2222.95820406   2236.74091153   2265.00474362   2286.0109957\n",
      "   2320.30111803   2365.25638019   2367.43878287   2375.98891897\n",
      "   2404.6104943    2406.26084635   2440.67674962   2487.67726518\n",
      "   2492.01855538   2555.13343235   2574.01005681   2607.41338625\n",
      "   2644.97346168   2654.64102833   2708.30382271   2722.57562081\n",
      "   2757.85746292   2827.86818504   2833.00885015   2869.47747821\n",
      "   2938.74425526   2959.48708908   3042.4813618    3049.21758549\n",
      "   3146.06409057   3185.3980744    3343.88418298   3349.71852381\n",
      "   3512.31722967   3672.02246224   3750.65757624   3765.24932165\n",
      "   3938.43604347   3966.81655637   3994.44383226   4254.50103815\n",
      "   4349.39774176   4975.03988133   5167.18625707   5253.95711417\n",
      "   5321.28903275   5643.48957946   5684.34435401   6100.78325008\n",
      "   6202.92130897   6225.44498493   6695.6237217    6909.25451295\n",
      "   6991.95319262   7592.18616124   8676.59147327   8834.0627301\n",
      "   9010.18684982   9233.67824451   9950.14483899  12275.77573554\n",
      "  13354.57225495  31062.27586415  33757.92767488  34986.73411255\n",
      "  36903.68315386  39057.094099    46323.41051233  46350.3361298\n",
      "  90579.04749761 202653.66990352 401462.84314133 845775.15155862]\n",
      "计算了200个奇异值\n",
      "U.shape(201626, 200)\n",
      "总共有200个非零奇异值\n",
      "选取了100个奇异值\n",
      "选取奇异值之和2134409.1421341794\n",
      "全部奇异值之和: 2272916.927539876\n",
      "选取的奇异值之和与全部奇异值之和的比例: 0.94\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from scipy.sparse import csr_matrix,coo_matrix,load_npz,save_npz\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "class SVDEmbedding:\n",
    "    \"\"\"\n",
    "    SVD分解的类，包括了数据预处理，共现矩阵的创建，SVD的分解\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 train_path: Optional[str]='../data/training.txt',\n",
    "                 k: Optional[int]=5,\n",
    "                 co_appear_matrix_path: Optional[str]=None,\n",
    "                 svd_vector_path: Optional[str]=None,\n",
    "                 num_dimensions: Optional[int]=200):\n",
    "        \"\"\"初始化\n",
    "\n",
    "        Args:\n",
    "            train_path (Optional[str], optional): 训练数据的路径. Defaults to '../data/training.txt'.\n",
    "            k (Optional[int], optional): 窗口大小. Defaults to 5.\n",
    "            co_appear_matrix_path (Optional[str], optional): 共现矩阵存储的路径. Defaults to None.\n",
    "            svd_vector_path (Optional[str], optional): SVD分解的路径. Defaults to None.\n",
    "            num_dimensions (Optional[int], optional): SVD维度. Defaults to 200.\n",
    "        \"\"\"\n",
    "        self.k = k  # 上下文窗口大小\n",
    "        self.word2id = {}  # 词到id的映射\n",
    "        self.id2word = {}  # id到词的映射\n",
    "        self.words_frequence = {}\n",
    "        self.num_dimensions = num_dimensions\n",
    "        \n",
    "        # 读取文本\n",
    "        self.words_list = self._read_data(train_path)\n",
    "        self.words_set = set(self.words_list)\n",
    "        print(len(self.words_set))\n",
    "        \n",
    "        if svd_vector_path is None:\n",
    "            # 构建共现矩阵  \n",
    "            if co_appear_matrix_path is None:\n",
    "                self.co_appear_matrix = None  # 共现矩阵\n",
    "                self._build_co_appear_matrix()\n",
    "                save_npz('co_appear_matrix.npz', self.co_appear_matrix)\n",
    "            else:\n",
    "                print('加载共现矩阵')\n",
    "                self.co_appear_matrix = load_npz(co_appear_matrix_path)\n",
    "            # SVD分解\n",
    "            self.word_vectors=None\n",
    "            self._svd_embedding()\n",
    "            \n",
    "        else:\n",
    "            print('加载svd embedding')\n",
    "            self.word_vectors = np.load(svd_vector_path)\n",
    "    def _read_data(self, \n",
    "                   train_path: Optional[str]='../data/training.txt'):  # 训练语料还是比较朴素的，只有一行，没有标点符号\n",
    "        \"\"\"读取数据，数据预处理\n",
    "\n",
    "        Args:\n",
    "            train_path (Optional[str], optional): 数据预处理. Defaults to '../data/training.txt'.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        # 初始化词形还原器\n",
    "        print('去除停用词和恢复词语原型')\n",
    "        stemmer = PorterStemmer()\n",
    "        with open(train_path, 'r') as f:\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            words_list = []\n",
    "            with open(train_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    words = line.split()  # list\n",
    "                    for word in words:\n",
    "                        # 如果单词不是停用词，才进行处理\n",
    "                        if word not in stop_words:\n",
    "                            word = stemmer.stem(word)\n",
    "                            words_list.append(word)\n",
    "                            if word not in self.word2id:\n",
    "                                \n",
    "                                word_id = len(self.word2id)\n",
    "                                self.word2id[word] = word_id\n",
    "                                self.id2word[word_id] = word\n",
    "        return words_list\n",
    "        \n",
    "\n",
    "    def _build_co_appear_matrix(self):\n",
    "        \"\"\"创建共现矩阵\n",
    "        \"\"\"\n",
    "        print('构建共现矩阵')\n",
    "        window_size = self.k\n",
    "        row=[]\n",
    "        col=[]\n",
    "        count=[]\n",
    "        window_size=5\n",
    "        num_words = len(self.words_set)  # 去重后的数量\n",
    "        print(f'num_words:{num_words}')\n",
    "        for i in range(len(self.words_list)) :\n",
    "            center_word=self.word2id[self.words_list[i]]\n",
    "            #在句子中的位置\n",
    "            window=list(range(max(0,i-window_size),min(i+1+window_size,len(self.words_list))))\n",
    "            window.remove(i)\n",
    "            for j in window :\n",
    "                #要转换为index\n",
    "                if center_word!=self.word2id[self.words_list[j]] :\n",
    "                    row.append(center_word)\n",
    "                    col.append(self.word2id[self.words_list[j]])\n",
    "                    count.append(1)\n",
    "        co_appear_matrix = coo_matrix((count, (row, col)), shape=(num_words,num_words),dtype=np.float64)\n",
    "        self.co_appear_matrix = co_appear_matrix\n",
    "        \n",
    "        \n",
    "    def _svd_embedding(self, \n",
    "                      save_path: Optional[str]='./svd_vector.npy'):\n",
    "        print('svd分解')\n",
    "        U, S, Vt = svds(self.co_appear_matrix,k=self.num_dimensions)\n",
    "        self.S = S\n",
    "        print(f'S{S.shape},{S}')\n",
    "        print(f'计算了{len(S)}个奇异值')\n",
    "        print(f'U.shape{U.shape}')\n",
    "        \n",
    "        non_zero_singular_values = len(S[S > 0])\n",
    "        print(f'总共有{non_zero_singular_values}个非零奇异值')\n",
    "        \n",
    "        select_demisions = 100 if int(non_zero_singular_values * 0.75)>100 else int(non_zero_singular_values * 0.75)\n",
    "        print(f'选取了{select_demisions}个奇异值')\n",
    "        \n",
    "        selected_sum = S[select_demisions:].sum()\n",
    "        print(f'选取奇异值之和{selected_sum}')\n",
    "        \n",
    "        all_sum = S.sum()\n",
    "        print(f'全部奇异值之和: {all_sum}')\n",
    "        \n",
    "        ratio = selected_sum / all_sum\n",
    "        print(f'选取的奇异值之和与全部奇异值之和的比例: {ratio:.2f}')\n",
    "        self.word_vectors = U[:, select_demisions:]\n",
    "        np.save(save_path,np.array(self.word_vectors))\n",
    "        \n",
    "    def load_vector(self):\n",
    "        return self.word_vectors\n",
    "\n",
    "    def get_cos_sim(self, word1, word2):\n",
    "        \"\"\"\n",
    "        计算传入的余弦相似度，如果有一个单词不在词典中，就返回0\n",
    "        \"\"\"\n",
    "        word1 = word1.lower()  # 将单词转换为小写\n",
    "        word2 = word2.lower()  # 将单词转换为小写\n",
    "\n",
    "        if (word1 not in self.word2id or word2 not in self.word2id):\n",
    "            return 0\n",
    "\n",
    "        word1_vec = self.word_vectors[self.word2id[word1]]\n",
    "        word2_vec = self.word_vectors[self.word2id[word2]]\n",
    "        cos_sim = np.dot(word1_vec, word2_vec) / (np.linalg.norm(word1_vec) * np.linalg.norm(word2_vec))\n",
    "        return cos_sim\n",
    "  \n",
    "        \n",
    "svd = SVDEmbedding(co_appear_matrix_path='/home/wangtuo/workspace/Homework/embedding/src/co_appear_matrix.npz')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201626\n",
      "(201626, 100)\n"
     ]
    }
   ],
   "source": [
    "vectors = svd.load_vector()\n",
    "print(len(svd.words_set))\n",
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txt_sim(test_txt_path='/home/wangtuo/workspace/Homework/embedding/data/test.txt', output_path='../data/output.txt'):\n",
    "    stemmer = PorterStemmer()\n",
    "    with open(test_txt_path, 'r') as f_in, open(output_path, 'w') as f_out:\n",
    "        for line in f_in:  # 对于每一行\n",
    "            line = line.strip()\n",
    "            words = line.split()\n",
    "            word1 = stemmer.stem(words[1])\n",
    "            word2 = stemmer.stem(words[2])\n",
    "            sim = svd.get_cos_sim(word1.lower(), word2.lower())  # 假设第一个词和第二个词分别是 words[0] 和 words[1]\n",
    "            line_with_sim = line + '\\t' + str(sim) + '\\n'  # 在行末加一个制表符后写入余弦相似度\n",
    "            f_out.write(line_with_sim)  # 将处理后的行写入输出文件\n",
    "\n",
    "# 调用函数，并指定输入文件路径和输出文件路径\n",
    "get_txt_sim('/home/wangtuo/workspace/Homework/embedding/data/test.txt', 'output1.txt')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
